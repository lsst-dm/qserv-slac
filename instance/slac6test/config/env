#!/bin/bash

#set -x

# This instance-specific script is supposed to be sourced from the Qserv
# management scripts in order to set up proper values of the corresponding
# parameters.

# ---------------------------------------------------------------------
# 2023-11-06: changes to the configuration of all 15 nodes of the Qserv
#             cluster.
#
# The default location of the core dumps on the RHEL8 filesystem is at
#
#  CORE_DUMPS=/var/lib/systemd/coredump
#
# The files are stored here in the lz4 compressed format by the system
# as configured by
#
# % cat /proc/sys/kernel/core_pattern
#   |/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e
#
# This service has the following configuration
#
# % cat /etc/systemd/coredump.conf
# [Coredump]
# #Storage=external
# #Compress=yes
# #ProcessSizeMax=2G
# #ExternalSizeMax=2G
# #JournalSizeMax=767M
# #MaxUse=
# #KeepFree=
#
# There are a few problems with this configuration:
#   - limited capacity (a few GB of the destinaton folder which is mounted
#     to the root filesystem of a host)
#   - the files are owned by user 'root'
#   - the files are truncated at 2GB (which is way to low for Qserv processes)
#
# The following solution was applied at each node of the cluster:
#
# mkdir -p /zfspool/core-files
# chmod a+rwx /zfspool/core-files
# echo '/zfspool/core-files/core.%e.%p.%h.%t' > /proc/sys/kernel/core_pattern
#
# And the new location of the files is specified below:

CORE_DUMPS=/zfspool/core-files

# -----------------
# ----- Qserv -----
# -----------------

# 10.6.8 + updated SciSQL
#
# QSERV_DB_IMAGE_TAG="qserv/lite-mariadb:2022.7.1-rc1-21-g292b0f1ea"
#
# 2024-11-19: pgraded to MariaDB 11.4.4
#
QSERV_DB_IMAGE_TAG="qserv/lite-mariadb:2024.11.1-rc1-1-g36b8ad4a4"

# Using custom versions built for Qserv
#
PROXYSQL_IMAGE_TAG="qserv/proxysql:2.5.3"
CZAR_SSL_PROXY_IMAGE_TAG="qserv/mysql-proxy-ssl:latest"

# 2024-05-13: The latest production release that supports HTTP frontend
#             for submitting queries.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.4.1-rc1"
#
# 2024-08-05: Based on DM-45595. The development version with HTTPS Czar frontend migrated
#             to CPP-HTTPLIB and worker ingest services migrated to CPP-HTTPLIB (it's still HTTP).
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-7-gdeca4627d"
#
# 2024-08-13: Based on DM-45595. Fixed a possible race condition in the implementation of
#             the worker ingest manager that might be causing a deadlock.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-9-g8d5648bd8"
#
# 2024-08-28: Based on DM-45811 which adds extended query search criteria.
#             The ticket itself was temporary rebased against DM-45595 that
#             included worker ingest service migrated to CPP-HTTPLIB and a possible
#             fix/workaround for the deadlock in worker REST API when processing
#             vert large (and slow to pull) contribution requests.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-12-g561ef443d"
#
# 2024-09-05: Based on DM-45811 which adds extended query search criteria.
#             This version of the ticket is baed on "main".
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-5-g600c8ce29"
#
# 2024-09-05: Based on DM-45595 that replaces qhttp with CPP-HTTPLIB
#             as the worker ingest REST service
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-5-ge88e3f9b4"
#
# 2024-09-05: Based on DM-46017 that is meant to fix a possible deadlock
#             seen in the Qserv worker ingest service when large ingest
#             contributions are being processed by the service.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-7-gdc74eca80"
#
# 2024-09-07: Based on DM-45929 that is meant to implement worker-centric view
#             of the ingest contributions on the Qserv Web Dashboard.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-10-gbc795c72b"
#
# 2024-09-17: Based on DM-46320 that adds support for the multipart/form-data format
#             in the Qserv's push-mode ingest protocol.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.7.1-rc1-14-g28383404a"
#
# 2024-10-04: The latest production release that includes all development tags
#             mentioned above.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.10.1-rc1"
#
# 2024-11-19: Based on DM-47512 (MariaDB 11.4.4) which is based on
#             the latest production release 2024.11.1-rc1
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.11.1-rc1-4-g4e4736078"
#
# 2024-11-20: Based on DM-47612 for refactoring and simplification of the Replication
#             worker which is also based on release 2024.11.2 (MariaDB 11.4.4).
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.11.2-rc1-2-ge42b22bd5"
#
# 2024-12-02: Based on DM-47748 (Refactor and simplify the Replication Controller Framework)
#             which itself is based on the main branch that also includes the above-mentioned
#             branch DM-47612.
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.11.2-rc1-4-g7f76d17ea"
#
# 2024-12-05: Based on DM-47985 (An option to control XROOTD/SSI chunk resource management
#             at Qserv workers) which is based on the "main" branch, which already includes
#             the branch DM-47748.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.11.2-rc1-8-g5988e6ace"
#
# 2025-01-07: Based on DM-42005 (which adds the HTTP-based Replication worker service).
#             The ticket includes the previous branches merged into "main".
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.11.2-rc1-18-gb5b2f7ff9"
#
# 2025-01-28: Based on DM-48646 (Replication worker crashes when no response from DNS is received),
#             that is based on "main". NOte this branch does NOT include the above-mentioned
#             branch DM-42005. So, no REST API here for the Replication worker services.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.11.2-rc1-25-g3d14871f1"
#
# 2025-02-03: Based on DM-48758 (improved implementation of SHOW [FULL] PROCESSLIST).
#             The tag includes the previous one that was merged earlier into the "main" branch.
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2024.11.2-rc1-30-gae2ac0897"
#
# 2025-02-09: Based on DM-48846 (sequrity patch in worker REST service) which is based
#             on 2025.2.2-rc1 end which also included the previous branch merged
#             into "main".
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2025.2.2-rc1-dirty"
#
#             Same as befor, except cashing FQDN in Task.cc to see if this would accelerate
#             queries
#
# QSERV_IMAGE_TAG="qserv/lite-qserv:2025.2.2-rc1-4-gb23e5eeeb"
#
# Adds tracking when Finished() is called in the QueryRequest
QSERV_IMAGE_TAG="qserv/lite-qserv:2025.2.2-rc1-23-g021a07aa0-dirty"

QSERV_BASE_DIR="/zfspool/qserv-test"
QSERV_CZAR_DB_PORT=3308
QSERV_CZAR_PROXY_PORT=4140
QSERV_CZAR_HTTP_PORT=4148
QSERV_WORKER_DB_PORT=3368
QSERV_XROOTD_PORT=1894
# These two ports are configured in $QSERV_BASE_DIR/master/config/proxysql.cnf
# Thea are not used by none of the deployment management scripts.
# Port QSERV_CZAR_PROXYSQL_USER_PORT is the SSL-encrypted fronend for
# Czar's port QSERV_CZAR_PROXY_PORT.
QSERV_CZAR_PROXYSQL_USER_PORT=14140
QSERV_CZAR_PROXYSQL_ADMIN_PORT=14141
QSERV_CZAR_HTTP_THREADS=16
QSERV_CZAR_HTTP_INGEST_THREADS=16
QSERV_CZAR_HTTP_CONFIG="/config-etc/qserv-czar-http.cfg"
QSERV_CZAR_HTTP_SSL_CERT="/config-etc/ssl/czar-cert.pem"
QSERV_CZAR_HTTP_SSL_KEY="/config-etc/ssl/czar-key.pem"
QSERV_CZAR_HTTP_TMP_DIR="/qserv/data/ingest"
QSERV_CZAR_HTTP_PARAMS="--config=${QSERV_CZAR_HTTP_CONFIG} --port=${QSERV_CZAR_HTTP_PORT} --threads=${QSERV_CZAR_HTTP_THREADS} --worker-ingest-threads=${QSERV_CZAR_HTTP_INGEST_THREADS} --ssl-cert-file=${QSERV_CZAR_HTTP_SSL_CERT} --ssl-private-key-file=${QSERV_CZAR_HTTP_SSL_KEY} --tmp-dir=${QSERV_CZAR_HTTP_TMP_DIR} --verbose"

# An alternative option for the TLS/SSL termination of the port
QSERV_CZAR_SSL_PROXY_PORT=14140

# -------------------------------------
# ----- Replication/Ingest system -----
# -------------------------------------

# 10.6.5
#
# REPL_DB_IMAGE_TAG="qserv/lite-mariadb:2022.1.1-rc1"
#
# 2024-11-19: pgraded to MariaDB 11.4.4
#
REPL_DB_IMAGE_TAG="qserv/lite-mariadb:2024.11.1-rc1-1-g36b8ad4a4"


# 2024-05-13: The latest production release that supports HTTP frontend
#             for submitting queries.
#
# REPL_IMAGE_TAG="qserv/lite-qserv:2024.4.1-rc1"
#
# 2024-08-05: Same tag as for Qserv.
#
REPL_IMAGE_TAG=$QSERV_IMAGE_TAG

REPL_INSTANCE_ID="slac6test"
REPL_DB_PORT=23308
REPL_CONTR_HTTP_PORT=25881
REPL_REG_PORT=25882

REPL_REG_PARAMETERS="--registry-port=${REPL_REG_PORT} --debug"

# Extended Controller timeouts to allow long operation when ingesting
# large catalogs:
#   request timeout: 8 hours
#   job timeout: 24 hours
# --controller-num-director-index-connections=4
#
# DISABLED (this option only exists in John's "feature" branch for his version of the "new" Qserv):
#   --qserv-chunk-map-update
#
# DISABLED (this flag and the option are only usefull for John's branch to disable replica
# synchronization mechanism):
#   --qserv-sync-disable
#   --xrootd-auto-notify=0
#
REPL_CONTR_PARAMETERS="--registry-port=${REPL_REG_PORT} --xrootd-port=${QSERV_XROOTD_PORT} --controller-http-server-port=${REPL_CONTR_HTTP_PORT} --http-root=/usr/local/qserv/www/ --worker-evict-timeout=3600 --health-probe-interval=120 --replication-interval=600 --qserv-sync-timeout=600 --do-not-create-folders --controller-ingest-job-monitor-ival-sec=1 --controller-request-timeout-sec=28800 --controller-job-timeout-sec=86400 --purge --debug"

REPL_WORKER_SVC_PORT=25800
REPL_WORKER_FS_PORT=25801
REPL_WORKER_LOADER_PORT=25802
REPL_WORKER_EXPORTER_PORT=25803
REPL_WORKER_HTTP_LOADER_PORT=25804
REPL_WORKER_HTTP_SVC_PORT=25805

# Uncomment to configure properly the HTTP-based Replication worker
REPL_WORKER_PARAMETERS="--registry-port=${REPL_REG_PORT} --worker-svc-port=${REPL_WORKER_SVC_PORT} --worker-fs-port=${REPL_WORKER_FS_PORT} --worker-loader-port=${REPL_WORKER_LOADER_PORT} --worker-exporter-port=${REPL_WORKER_EXPORTER_PORT} --worker-http-loader-port=${REPL_WORKER_HTTP_LOADER_PORT} --worker-ingest-num-retries=4 --schema-upgrade-wait=1 --schema-upgrade-wait-timeout=600 --do-not-create-folders --worker-num-async-loader-processing-threads=32 --worker-num-http-loader-processing-threads=32 --worker-http-svc-port=${REPL_WORKER_HTTP_SVC_PORT} --debug"
# REPL_WORKER_PARAMETERS="--registry-port=${REPL_REG_PORT} --worker-svc-port=${REPL_WORKER_SVC_PORT} --worker-fs-port=${REPL_WORKER_FS_PORT} --worker-loader-port=${REPL_WORKER_LOADER_PORT} --worker-exporter-port=${REPL_WORKER_EXPORTER_PORT} --worker-http-loader-port=${REPL_WORKER_HTTP_LOADER_PORT} --worker-ingest-num-retries=4 --schema-upgrade-wait=1 --schema-upgrade-wait-timeout=600 --do-not-create-folders --worker-num-async-loader-processing-threads=32 --worker-num-http-loader-processing-threads=32 --debug"


# Uncomment if needed to mount the local (ouside the container) version of
# the Web dashboard and other Web apps served by the HTTP server of
# the Master Controller 
#
# REPL_HTTP_ROOT="/zfspool/management/qserv/src/www"

# Limit size of the replication containers by 100 GB
# This is needed for ingesting (very) large numbers of contributions.
REPL_MEMORY_LIMIT=107374182400

# ----- Common parameters

# User account under which the containers will be run.
#
# % id rubinqsv
#   uid=45386(rubinqsv) gid=1126(gu) groups=1126(gu)
#
CONTAINER_UID=45386
CONTAINER_GID=1126

CONTAINER_NAME_PREFIX="qserv-test-"
#CONTAINER_RESTART_POLICY="unless-stopped"
CONTAINER_RESTART_POLICY=""

# 10 GB
ULIMIT_MEMLOCK=1099511627776

# Unlimited core file size
ULIMIT_CORE="-1:-1"

# Other parameters are set by the following script based on the above defined
# one, or be loaded from predefined locations.
. $(dirname "$0")/common

